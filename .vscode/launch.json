{
    "version": "0.2.0",
    "configurations": [
      {
        "name": "Python: Debug example_chat_completion.py with torchrun",
        "type": "python",
        "request": "launch",
        "program": "${workspaceFolder}/.venv/bin/torchrun",
        "console": "integratedTerminal",
        "env": {
          "PYTORCH_ENABLE_MPS_FALLBACK": "1",
        },
        "args": [
          "--nproc_per_node",
          "1",
          "${workspaceFolder}/example_chat_completion.py",
          "--ckpt_dir",
          "Meta-Llama-3-8B-Instruct/",
          "--tokenizer_path",
          "Meta-Llama-3-8B-Instruct/tokenizer.model",
          "--max_seq_len",
          "512",
          "--max_batch_size",
          "6"
        ]
      }
    ]
  }